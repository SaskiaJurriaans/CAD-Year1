---
title: "Bayes_moore_Yield"
edits: "Edits made in scripts following meeting with Murray 17/04/2024"
author: "Sas"
format: html
editor: visual
---

# Preparations

Load the necessary libraries

```{r}
#| label: libraries
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: false

library(tidyverse)  #for data wrangling etc
library(rstanarm)   #for fitting models in STAN
#library(cmdstanr)   #for cmdstan --> does not work on AIMS computers
library(brms)       #for fitting models in STAN
library(standist)   #for exploring distributions
library(coda)       #for diagnostics
library(bayesplot)  #for diagnostics
library(ggmcmc)     #for MCMC diagnostics
library(DHARMa)     #for residual diagnostics
library(rstan)      #for interfacing with STAN
library(emmeans)    #for marginal means etc
library(broom)      #for tidying outputs
library(tidybayes)  #for more tidying outputs
library(HDInterval) #for HPD intervals
library(ggeffects)  #for partial plots
library(broom.mixed)#for summarising models
library(posterior)  #for posterior draws
library(ggeffects)  #for partial effects plots
library(patchwork)  #for multi-panel figures
library(bayestestR) #for ROPE
library(see)        #for some plots
library(readxl)     #to load excel documents
library(easystats)     #framework for stats, modelling and visualisation
#library(INLA)       #for approximate Bayes
library(openxlsx)    # to write excel documents
#library(INLAutils)  #for additional INLA outputs
theme_set(theme_grey()) #put the default ggplot theme back
source('helperFunctions.R')
```

# Read in the data

```{r}
survival <- read_excel("data/YEAR1_Survival.xlsx", na ="")

environment <- read_excel ("data/YEAR1 Benthic Environment ReefDev.xlsx", na="")
```

# Exploratory data

```{r}
glimpse(survival)
head(survival)
str(survival)
survival |> datawizard::data_codebook()
```

Remove some unnecessary columns

```{r}
survival <- survival |>
  select(-c(ID, Observer, CensusT, DeploymentDate, CensusDate, Device_ID, NoRecruits))
```

# Moore Yield

### Subset data: moore yield (device level)

```{r}
moore_surv <- survival |> filter (Reef == "Moore",
                             Census == "t6",
                             Tab_ID == 1) |>
  select(-c(Tab_ID, SurvTab, Spp))

moore_env <- environment |> 
  filter (Reef == "Moore") |>
  select(-c(Reef1, Device_ID))
```

### Merge survival and environment

```{r}
moore_t5 <- merge(moore_env, moore_surv, by = c("Reef", "Site", "ReefDev"), all = TRUE)

#free up space
remove(moore_surv, moore_env)

# Remove rows with missing values
moore_t5$SurvDev <- as.numeric(moore_t5$SurvDev)
moore_t5 <- moore_t5[complete.cases(moore_t5$SurvDev),  ]
moore_t5 <- moore_t5[complete.cases(moore_t5$PC1),  ]
moore_t5 <- moore_t5 |> select(-c(sedturf_t2, sedconcrete_t2))
```

### GLM

```{r}
library(lme4)
moore_t5$Site <- as.factor(moore_t5$Site)
moore_t5$ReefDev<- as.factor(moore_t5$ReefDev)

moore_glmer1<- glmer(SurvDev ~ (1|Site) + (1|ReefDev), data=moore_t5, family = binomial(link = "logit")) #allowing for random intercepts but not random slopes --> accounting for potential variability in survival beteeen different sites and devices, but assuming that the effect of device on  survival is constant across different sites.

moore_glmer2<- glmer(SurvDev ~ Ub_avrg + (1|Site) + (1|ReefDev), data=moore_t5, family = binomial(link = "logit")) 
moore_glmer3<- glmer(SurvDev ~ WaveEnergyLevel + (1|Site) + (1|ReefDev), data=moore_t5, family = binomial(link = "logit")) 
moore_glmer4<- glmer(SurvDev ~ median_speed + (1|Site) + (1|ReefDev), data=moore_t5, family = binomial(link = "logit")) 
moore_glmer5<- glmer(SurvDev ~ sedturf_t5 + (1|Site) + (1|ReefDev), data=moore_t5, family = binomial(link = "logit")) 

summary(moore_glmer1)
summary(moore_glmer2)
summary(moore_glmer3)
summary(moore_glmer4)
summary(moore_glmer5)

AIC(moore_glmer1, moore_glmer2, moore_glmer3, moore_glmer4, moore_glmer5)
```

OUTPUT: all models behave very similar

### Visualize data

### **median_speed**

```{r}
ggplot(data=moore_t5, aes(y = SurvDev, x = median_speed)) + 
  geom_point(position = position_jitter(width = 0.2, height = 0)) +
  geom_smooth(method="lm")

ggplot(data=moore_t5, aes(y = SurvDev, x = median_speed)) + 
  geom_point(position = position_jitter(width = 0.2, height = 0)) +
  geom_smooth(method="lm", formula = y~poly(x,3), se = FALSE)

#with facetwrap by site
ggplot(data=moore_t5, aes(y = SurvDev, x = median_speed)) + 
  geom_point(position = position_jitter(width = 0.2, height = 0)) +
  facet_wrap(~Site) +
  geom_smooth(method="lm")
```

Wave Energy Level

```{r}
ggplot(data=moore_t5, aes(y = SurvDev, x = WaveEnergyLevel)) + 
  geom_point(position = position_jitter(width = 0.2, height = 0)) +
  geom_smooth(method="lm")

ggplot(data=moore_t5, aes(y = SurvDev, x = WaveEnergyLevel)) + 
  geom_point(position = position_jitter(width = 0.2, height = 0)) +
  geom_smooth(method="lm", formula = y~poly(x,3), se = FALSE)

#with facetwrap by site
ggplot(data=moore_t5, aes(y = SurvDev, x = WaveEnergyLevel)) + 
  geom_point(position = position_jitter(width = 0.2, height = 0)) +
  facet_wrap(~Site) +
  geom_smooth(method="lm")
```

# Brm

We need to make sure that the categorical variables and the random effects are defined as factors:

```{r}
moore_t5 <- moore_t5 |> mutate(Site = factor(Site),
                               ReefDev = factor(ReefDev))
```

### Formulas

Logistic mixed-effects regression model with random intercepts for Site and Device:

```{r}
moore_form <- bf(SurvDev ~ (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_speed <- bf(SurvDev ~ median_speed +    (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_Ub <-    bf(SurvDev ~ Ub_avrg +         (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_turf <-   bf(SurvDev ~ sedturf_t5 +     (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_concrete <-bf(SurvDev ~ sedconcrete_t5 +(1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_Wave <-  bf(SurvDev ~ WaveEnergyLevel + (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_PC1 <-   bf(SurvDev ~ PC1 +             (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
```

Now with polynomial terms:

```{r}
moore_form <- bf(SurvDev ~ (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_speed_poly <- bf(SurvDev ~ poly(median_speed,3) +    (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_Ub_poly <-    bf(SurvDev ~ poly(Ub_avrg,3) +         (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_turf_poly <-   bf(SurvDev ~ poly(sedturf_t5,3) +      (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_Wave_poly <-  bf(SurvDev ~ poly(WaveEnergyLevel,3) + (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
moore_form_PC1_poly <-   bf(SurvDev ~ poly(PC1,3) +             (1|Site) + (1|ReefDev), family = bernoulli(link='logit'))
```

### Priors

class "Intercept" -\> for random effects

class "b" -\> for regression coefficent

class "sd" -\> for standard deviations of random effects

```{r}
priors <- prior(normal(0, 1.7), class = "Intercept") +
   prior(student_t(3, 0, 1.5), class = 'sd')
```

**median speed**

```{r}
2.5 * sd(moore_t5$SurvDev, na.rm = TRUE)/sd(moore_t5$median_speed, na.rm = TRUE)

priors_speed <- prior(normal(0, 1.7), class = "Intercept") +
  prior(normal(0,57), class = "b")  +
   prior(student_t(3, 0, 1.5), class = 'sd')
```

**Ub**

```{r}
2.5 * sd(moore_t5$SurvDev, na.rm = TRUE)/sd(moore_t5$Ub_avrg, na.rm = TRUE)

priors_Ub <- prior(normal(0, 1.7), class = "Intercept") +
  prior(normal(0,8), class = "b")  +
   prior(student_t(3, 0, 1.5), class = 'sd')
```

**turf**

```{r}
2.5 * sd(moore_t5$SurvDev, na.rm = TRUE)/sd(moore_t5$sedturf_t5, na.rm = TRUE)

priors_turf <- prior(normal(0, 1.7), class = "Intercept") +
  prior(normal(0,1.6), class = "b")  +
   prior(student_t(3, 0, 1.5), class = 'sd')
```

**concrete**

```{r}
2.5 * sd(moore_t5$SurvDev, na.rm = TRUE)/sd(moore_t5$sedconcrete_t5, na.rm = TRUE)

priors_concrete <- prior(normal(0, 1.7), class = "Intercept") +
  prior(normal(0,4), class = "b")  +
   prior(student_t(3, 0, 1.5), class = 'sd')
```

**WaveEnergyLevel**

```{r}
2.5 * sd(moore_t5$SurvDev, na.rm = TRUE)/sd(moore_t5$WaveEnergyLevel, na.rm = TRUE)

priors_wave <- prior(normal(0, 1.7), class = "Intercept") +
  prior(normal(0,1), class = "b")  +
   prior(student_t(3, 0, 1.5), class = 'sd')

```

**PC1**

```{r}
2.5 * sd(moore_t5$SurvDev, na.rm = TRUE)/sd(moore_t5$PC1, na.rm = TRUE)

priors_PC1 <- prior(normal(0, 1.7), class = "Intercept") +
  prior(normal(0,8), class = "b")  +
   prior(student_t(3, 0, 1.5), class = 'sd')
```

### brm- Random effects only

```{r}
moore_brm <- brm(moore_form,
                 data=moore_t5,
                 prior = priors,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.99),
                backend = "rstan")

# To save the model use >> 
#save(moore_brm, file = "scripts/models/moore_brm.RData")
```

### brm- speed

```{r}
moore_brm_speed <- brm(moore_form_speed,
                 data=moore_t5,
                 prior = priors_speed,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.9),
                 backend = "rstan")

# Save the model
#save(moore_brm_speed, file = "scripts/models/moore_brm_speed.RData")
```

### brm- Ub

```{r}
moore_brm_Ub <- brm(moore_form_Ub,
                 data=moore_t5,
                 prior = priors_Ub,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.9),
                backend = "rstan")

# Save the model
#save(moore_brm_Ub, file = "scripts/models/moore_brm_Ub.RData")
```

### brm- wave

```{r}
moore_brm_wave <- brm(moore_form_Wave,
                 data=moore_t5,
                 prior = priors_wave,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.99),
                backend = "rstan")

# Save the model
#save(moore_brm_wave, file = "scripts/models/moore_brm_wave.RData")
```

### brm- turf

```{r}
moore_brm_turf <- brm(moore_form_turf,
                 data=moore_t5,
                 prior = priors_turf,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.9),
                backend = "rstan")

# Save the model 
#save(moore_brm_turf, file = "scripts/models/moore_brm_turf.RData")
```

### brm- concrete

```{r}
moore_brm_concrete <- brm(moore_form_concrete,
                 data=moore_t5,
                 prior = priors_concrete,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.9),
                backend = "rstan")

# Save the model 
#save(moore_brm_concrete, file = "scripts/models/moore_brm_concrete.RData")
```

### brm- PC1

```{r}
moore_brm_PC1 <- brm(moore_form_PC1,
                 data=moore_t5,
                 prior = priors_PC1,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores = 3,
                 thin = 5,
                 refresh = 0,
                 control = list(adapt_delta = 0.9),
                backend = "rstan")

# Save the model 
#save(moore_brm_PC1, file = "scripts/models/moore_brm_PC1.RData")
```

### Load models

```{r}
load(file = "scripts/models/moore_brm.RData")
load(file = "scripts/models/moore_brm_speed.RData")
load(file = "scripts/models/moore_brm_Ub.RData")
load(file = "scripts/models/moore_brm_wave.RData")
load(file = "scripts/models/moore_brm_turf.RData")
load(file = "scripts/models/moore_brm_concrete.RData")
load(file = "scripts/models/moore_brm_PC1.RData")
```

### loo

```{r}
#Using loo compare, lower value is better
l_brm <- moore_brm |> loo()
l_speed <- moore_brm_speed|> loo()
l_Ub <- moore_brm_Ub |> loo()
l_wave <- moore_brm_wave |> loo()
l_turf <- moore_brm_turf|> loo()
l_concrete <- moore_brm_concrete|> loo()
l_PC1 <- moore_brm_PC1|> loo()

loo_compare(l_brm, l_speed, l_Ub, l_wave, l_turf, l_concrete, l_PC1)
```

NOTES ON THIS: Prefer the model with the lower LOOIC, as it suggests better expected out-of-sample predictive performance. However, consider the uncertainty (SE) around these estimates. Significance of Differences: Look at the differences in LOOIC and their standard errors. A rule of thumb is that a difference greater than twice its standard error might be considered substantial, but this is not a strict threshold. If two models have similar LOOIC values, the simpler model (the one with fewer parameters) is typically preferred due to the principle of parsimony. This is not directly shown in the loo_compare() output but is an important consideration when interpreting results.

# Diagnostics

### brm

#### **MCMC**

```{r}
#pars <- moore_brm |> get_variables()  #capture the list of variables
pars <- str_extract(pars, "^b_.*|^sd.*") |> na.omit()
pars

moore_brm $fit |> stan_trace(pars = pars)
moore_brm $fit |> stan_ac(pars = pars)
moore_brm$fit |> stan_rhat()
moore_brm$fit |> stan_ess()  # the closer to 1, the more chains have converged
moore_brm |> mcmc_plot(type='violin')  # assess convergence and mixing of MCMC samples and skewness of the distribution
```

-   **stan_ac**: All below 0.25, all good.
-   **rhat_hist**: Rhat is a **scale reduction factor** measure of convergence between the chains. The closer the values are to 1, the more the chains have converged. Values greater than 1.05 indicate a lack of convergence. There will be an Rhat value for each parameter estimated.
-   **stan_ess:** effective sample size not very efficient, most below 50%.

#### **Posterior probability checks**

```{r}
moore_brm |> pp_check(type = 'dens_overlay', ndraws = 100)
moore_brm |> pp_check (type = "error_scatter_avg")
moore_brm  |> pp_check(group = 'Site', type = 'violin_grouped')
```

-   **dens_overlay**: plots the density distribution of the observed data (black line) overlayed ontop of 50 density distributions generated from draws from the model (light blue). Ideally, the 50 realisations should be roughly consistent with the observed data.
-   **error_scatter_avg**: this plots the observed values against the average residuals. Similar to a residual plot, we do not want to see any patterns in this plot. Note, this is not really that useful for models that involve a binomial response
-   **intervals**: plots the observed data overlayed ontop of posterior predictions associated with each level of the predictor. Ideally, the observed data should all fall within the predictive intervals.

#### **dHARMA residuals**

```{r}
library(DHARMa)
moore.resids <- make_brms_dharma_res(moore_brm, integerResponse = FALSE)
testUniformity(moore.resids)
```

```{r}
wrap_elements(~testUniformity(moore.resids))
wrap_elements(~plotResiduals(moore.resids, form = factor(rep(1,nrow(moore_t5))))) 
wrap_elements(~plotResiduals(moore.resids, quantreg = TRUE))   # leave the quartile calculations off if you have a small amount of data
wrap_elements(~testDispersion(moore.resids))
```

**Conclusions:**

-   the simulated residuals do not suggest any issues with the residuals
-   there is no evidence of a lack of fit.

### brm- Speed

#### **MCMC**

```{r}
pars <- moore_brm_speed |> get_variables()  #capture the list of variables
pars <- str_extract(pars, "^b_.*|^sd.*") |> na.omit()
pars

moore_brm_speed$fit |> stan_trace(pars = pars)
moore_brm_speed$fit |> stan_ac(pars = pars)
moore_brm_speed$fit |> stan_rhat()
moore_brm_speed$fit |> stan_ess()
```

-   **stan_ac**: All below 0.25, all good.
-   **rhat_hist**: Rhat is a **scale reduction factor** measure of convergence between the chains. The closer the values are to 1, the more the chains have converged. Values greater than 1.05 indicate a lack of convergence. There will be an Rhat value for each parameter estimated.
-   **stan_ess:** effective sample size better than previous model, more around 70%

#### **Posterior probability checks**

```{r}
moore_brm_speed |> pp_check(type = 'dens_overlay', ndraws = 100)
moore_brm_speed |> pp_check (type = "error_scatter_avg")
moore_brm_speed |> mcmc_plot(type='violin')
```

#### **dHARMA residuals**

```{r}
library(DHARMa)
moore.resids <- make_brms_dharma_res(moore_brm_speed, integerResponse = TRUE)
testUniformity(moore.resids)
```

```{r}
#remove NA values of predictor variable
moore_t5 <- moore_t5[complete.cases(moore_t5$median_speed),  ]

wrap_elements(~testUniformity(moore.resids))
wrap_elements(~plotResiduals(moore.resids, form = factor(rep(1,nrow(moore_t5))))) 
wrap_elements(~plotResiduals(moore.resids, quantreg = TRUE))   # leave the quartile calculations off if you have a small amount of data
wrap_elements(~testDispersion(moore.resids))
```

### brm- Ub

#### **MCMC**

```{r}
pars <- moore_brm_Ub |> get_variables()  #capture the list of variables
pars <- str_extract(pars, "^b_.*|^sd.*") |> na.omit()
pars

moore_brm_Ub$fit |> stan_trace(pars = pars)
moore_brm_Ub$fit |> stan_ac(pars = pars)
moore_brm_Ub$fit |> stan_rhat()
moore_brm_Ub$fit |> stan_ess()
```

#### **Posterior probability checks**

```{r}
moore_brm_Ub |> pp_check(type = 'dens_overlay', ndraws = 100)
moore_brm_Ub |> pp_check (type = "error_scatter_avg")
moore_brm_Ub |> mcmc_plot(type='violin')
```

#### **dHARMA residuals**

```{r}
library(DHARMa)
moore.resids <- make_brms_dharma_res(moore_brm_Ub, integerResponse = TRUE)
testUniformity(moore.resids)
```

```{r}
wrap_elements(~testUniformity(moore.resids))
wrap_elements(~plotResiduals(moore.resids, form = factor(rep(1,nrow(moore_t5))))) 
wrap_elements(~plotResiduals(moore.resids, quantreg = TRUE))   # leave the quartile calculations off if you have a small amount of data
wrap_elements(~testDispersion(moore.resids))
```

### brm- Wave

#### **MCMC**

```{r}
pars <- moore_brm_wave |> get_variables()  #capture the list of variables
pars <- str_extract(pars, "^b_.*|^sd.*") |> na.omit()
pars

moore_brm_wave$fit |> stan_trace(pars = pars)
moore_brm_wave$fit |> stan_ac(pars = pars)
moore_brm_wave$fit |> stan_rhat()
moore_brm_wave$fit |> stan_ess()
```

#### **Posterior probability checks**

```{r}
moore_brm_wave |> pp_check(type = 'dens_overlay', ndraws = 100)
moore_brm_wave |> pp_check (type = "error_scatter_avg")
moore_brm_wave |> mcmc_plot(type='violin')
```

#### **dHARMA residuals**

```{r}
library(DHARMa)
moore.resids <- make_brms_dharma_res(moore_brm_wave, integerResponse = FALSE)
testUniformity(moore.resids)
```

```{r}
wrap_elements(~testUniformity(moore.resids))
wrap_elements(~plotResiduals(moore.resids, form = factor(rep(1,nrow(moore_t5))))) 
wrap_elements(~plotResiduals(moore.resids, quantreg = TRUE))   # leave the quartile calculations off if you have a small amount of data
wrap_elements(~testDispersion(moore.resids))
```

### brm - turf

#### **MCMC**

```{r}
pars <- moore_brm_turf |> get_variables()  #capture the list of variables
pars <- str_extract(pars, "^b_.*|^sd.*") |> na.omit()

moore_brm_turf$fit |> stan_trace(pars = pars)
moore_brm_turf$fit |> stan_ac(pars = pars)
moore_brm_turf$fit |> stan_rhat()
moore_brm_turf$fit |> stan_ess()
```

#### **Posterior probability checks**

```{r}
moore_brm_turf |> pp_check(type = 'dens_overlay', ndraws = 100)
moore_brm_turf |> pp_check (type = "error_scatter_avg")
moore_brm_turf |> mcmc_plot(type='violin')
```

#### **dHARMA residuals**

```{r}
library(DHARMa)
moore.resids <- make_brms_dharma_res(moore_brm_turf, integerResponse = FALSE)
testUniformity(moore.resids)
```

```{r}
wrap_elements(~testUniformity(moore.resids))
wrap_elements(~plotResiduals(moore.resids, form = factor(rep(1,nrow(moore_t5))))) 
wrap_elements(~plotResiduals(moore.resids, quantreg = TRUE))   # leave the quartile calculations off if you have a small amount of data
wrap_elements(~testDispersion(moore.resids))
```

### brm- PC1

#### **MCMC**

```{r}
pars <- moore_brm_PC1 |> get_variables()  #capture the list of variables
pars <- str_extract(pars, "^b_.*|^sd.*") |> na.omit()


moore_brm_PC1$fit |> stan_trace(pars = pars)
moore_brm_PC1$fit |> stan_ac(pars = pars)
moore_brm_PC1$fit |> stan_rhat()
moore_brm_PC1$fit |> stan_ess()
```

#### **Posterior probability checks**

```{r}
moore_brm_PC1 |> pp_check(type = 'dens_overlay', ndraws = 100)
moore_brm_PC1 |> pp_check (type = "error_scatter_avg")
moore_brm_PC1 |> mcmc_plot(type='violin')
```

#### **dHARMA residuals**

```{r}
library(DHARMa)
moore.resids <- make_brms_dharma_res(moore_brm_PC1, integerResponse = FALSE)
testUniformity(moore.resids)
```

```{r}
wrap_elements(~testUniformity(moore.resids))
wrap_elements(~plotResiduals(moore.resids, form = factor(rep(1,nrow(moore_t5))))) 
wrap_elements(~plotResiduals(moore.resids, quantreg = TRUE))   # leave the quartile calculations off if you have a small amount of data
wrap_elements(~testDispersion(moore.resids))
```

# Further analyses

## Visualization

```{r}
#moore_brm |> conditional_effects() |> plot(points=TRUE)
moore_brm_speed |> conditional_effects() |> plot(points=TRUE)
moore_brm_Ub |> conditional_effects() |> plot(points=TRUE)
moore_brm_turf |> conditional_effects() |> plot(points=TRUE)
moore_brm_wave |> conditional_effects() |> plot(points=TRUE)
moore_brm_PC1 |> conditional_effects() |> plot(points=TRUE)
```

### Figure 1

```{r partialPlot2h1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}

ggpredict(moore_brm_speed, terms = ~median_speed) %>%
   plot(add.data = TRUE)
 

Fig1a <- ggemmeans(moore_brm_speed, terms = ~median_speed) %>%
  plot(add.data = TRUE) 

conditional_effects(moore_brm_speed) |> 
  plot(points = TRUE)

Fig1b <- moore_brm_speed %>% emmeans(~median_speed , type = 'link') %>%
    as.data.frame() %>%
    ggplot(aes(y = emmean, x = median_speed)) +
   # geom_hline(yintercept = c(5,-5), linetype = 'dashed') +
    geom_pointrange(aes(ymin = lower.HPD, ymax = upper.HPD)) +
    geom_point(data = moore_t5, aes(y = SurvDev),
              # position = position_jitter(width=0.02, height = 0),
               alpha=0.4, color = 'red') +  
     labs(title = "moore Reef", x = "Median Speed (m/s)", y = "Estimated marginal mean - Survival (yield)") +  # Axis labels
  theme_minimal()  # Apply minimal theme

print(Fig1a)
print(Fig1b)

#ggsave("Fig1a - moore Predicted probabilities SurvDev.jpeg", Fig1a, width = 10, height = 6)
#ggsave("Fig1b - moore Estimated marginal means SurvDev.jpeg", Fig1b, width = 10, height = 6)
#saved on 05/04/2024


```

### Figure 2

```{r}
new_data <- data.frame(
  median_speed = median_speed_seq
)

# Extract fitted draws for the new data
#fitted_draws <- add_epred_draws(moore.speed.brm, newdata = new_data, allow_new_levels = TRUE)

fitted_draws <- moore.speed.brm |>
  add_fitted_draws(newdata = new_data, allow_new_levels = TRUE)

summary_stats <- fitted_draws |>
    summarise(
    mean_survival_probability = mean(.value), #calculating the predicted means
    se = sd(.value) / sqrt(n())  # Calculate standard error
  ) |>
  ungroup()


# Plotting the results
Fig2 <- ggplot(summary_stats, aes(x = median_speed_seq, y = mean_survival_probability, group = 1)) +
  geom_line() +  
  geom_point() +  
#  geom_errorbar(aes(ymin = mean_survival_probability - se, ymax = mean_survival_probability + se), 
#                width = 0.1) +  # Set the width of the error bars  
  labs( title = "moore Reef", y = "survival probability", x = "median speed (m/s)") +
  theme_bw() + theme(text = element_text(size = 12)) 

print(Fig2)
#ggsave("Fig2 - moore Survival probabilities.jpeg", Fig2, width = 10, height = 6)
#saved on 05/04/2024
```

# Interpretation

### brm

### speed

```{r}
moore_brm_speed |> summary()
```

```{r}
moore.speed.brm$fit |> tidyMCMC(estimate.method = "median", 
                            conf.int = TRUE, 
                            conf.method = "HPDinterval", 
                            rhat = TRUE, 
                            ess = TRUE, 
                            exponential = TRUE)
```

```{r}
moore_speed_tidy <- moore_brm_speed |> as_draws_df()  |>
  dplyr::select(matches("^b_.*")) |>    # a more sophisticated way of selecting (and searching) for pattersn in our column headers
  mutate(across(everything(), exp)) |>
  summarise_draws("median",            #  for every column calculate the median
                 ~HDInterval::hdi(.x), #  highest density intervals come from a specific package and threfore you need to specify the package it comes from, and therefore you need to put a ~ infront of it, and (.x) 
                  "rhat",               # for every column give me the rhat 
                  "ess_bulk",           # for every column give me the ess_bulk
                 P = ~mean(.x <1),    # what is the probability that there is any effect (100%)
                 P5 = ~mean(.x <0.95),  # what is the probability that there is a 5% decline?
                 P50= ~mean(.x <0.5)) # what is the probability that there is a 50% decline?
  
#RUN IN CONSOLE
#write.xlsx(moore.speed.brm.tidy, file = "data/moore survival-speed brm tidy_odds ratio.xlsx") #Saving this data for interpretation in a manuscript

```

[**This table is the easiest to describe as your narrative for the biology/ecology of your data.**]{.underline} Also we have now first done the exponential of each number and then summarized this, instead of first summarizing it and then calculating the exponential of it. Therefore, for the Confidence Interval of b_median_current_speed we are now looking for that this range [**does not exceed 1**]{.underline} (as the exponential of zero is 1). Does the interval of \[0.0000- 0.0001489\] includes 1? NO! Therefore we have evidence that our slope decreases with current

### Ub

```{r}
moore_brm_Ub |> summary()
```

### Wave

```{r}
moore_brm_wave |> summary()
```

### Turf

```{r}
moore_brm_turf |> summary()
```

### Concrete

```{r}
moore_brm_concrete |> summary()
```

### R2

```{r}
# Store models in a list
models <- list(
  moore_brm = moore_brm,
  moore_brm_speed = moore_brm_speed,
  moore_brm_wave = moore_brm_wave,
  moore_brm_Ub = moore_brm_Ub,
  moore_brm_turf = moore_brm_turf,
  moore_brm_concrete = moore_brm_concrete,
  moore_brm_PC1 = moore_brm_PC1
)

# Apply operations and store results in a list
results <- lapply(models, function(model) {
  model |>
    bayes_R2(summary = FALSE) |>
    median_hdci()
})

# Print the list of results
print(results)

```

```{r}
moore_brm_speed |> 
  as_draws_df() |>
  dplyr::select(matches("^b_.*")) |>
  mutate(LD50 = -1*b_Intercept / b_median_speed) |>
  median_hdci(LD50)
```
